{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19d8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f328e024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7043, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Load the dataset and explore the variables. √\n",
    "df = pd.read_csv('customer_churn.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee2623a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808802650260294"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) We will try to predict variable Churn using a logistic regression on variables tenure, SeniorCitizen,\n",
    "# MonthlyCharges. √\n",
    "# 3) Extract the target variable. √\n",
    "# 4) Extract the independent variables and scale them. √\n",
    "# 5) Build the logistic regression model. √\n",
    "# 6) Evaluate the model. √\n",
    "numericData = df[['tenure', 'SeniorCitizen','MonthlyCharges']]\n",
    "transformer = StandardScaler().fit(df[['tenure','SeniorCitizen','MonthlyCharges']])\n",
    "scaled_x = transformer.transform(df[['tenure','SeniorCitizen','MonthlyCharges']])\n",
    "\n",
    "#y = pd.DataFrame(data=df, columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_x, y, test_size=0.3, random_state=100)\n",
    "classification = LogisticRegression(random_state=0, multi_class='ovr').fit(X_train, y_train)\n",
    "predictions = classification.predict(X_test)\n",
    "\n",
    "classification.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c66194e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     5174\n",
       "Yes    1869\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) Even a simple model will give us more than 70% accuracy. Why? A: it is because of the imbalance of the data\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346be171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Synthetic Minority Oversampling TEchnique (SMOTE) is an over sampling technique based on nearest neighbors \n",
    "# that adds new points between existing points. Apply imblearn.over_sampling.SMOTE to the dataset. \n",
    "# Build and evaluate the logistic regression model. Is it there any improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff922b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn\n",
       "No       5174\n",
       "Yes      5174\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X = df[['tenure', 'SeniorCitizen','MonthlyCharges']]\n",
    "transformer = StandardScaler().fit(X)\n",
    "X = transformer.transform(X)\n",
    "y = pd.DataFrame(data=df, columns=['Churn'])\n",
    "X_sm, y_sm = smote.fit_resample(X, y)\n",
    "y_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5565587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.744927536231884"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=100)\n",
    "classification = LogisticRegression(random_state=0, multi_class='ovr').fit(X_train, y_train)\n",
    "predictions = classification.predict(X_test)\n",
    "\n",
    "#check accuracity\n",
    "classification.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0ddf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Tomek links are pairs of very close instances, but of opposite classes. Removing the instances \n",
    "# of the majority class of each pair increases the space between the two classes, \n",
    "# facilitating the classification process. Apply imblearn.under_sampling.TomekLinks to the dataset. \n",
    "# Build and evaluate the logistic regression model. Is it there any improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c75d300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn\n",
       "No       4694\n",
       "Yes      1869\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks('majority')\n",
    "X_tl, y_tl = tl.fit_resample(X, y)\n",
    "y_tl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b13562dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn\n",
       "No       4541\n",
       "Yes      1869\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tl2, y_tl2 = tl.fit_resample(X_tl, y_tl)\n",
    "y_tl2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "972258d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7973590655154901"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tl, y_tl, test_size=0.3, random_state=100)\n",
    "classification = LogisticRegression(random_state=0, multi_class='ovr').fit(X_train, y_train)\n",
    "predictions = classification.predict(X_test)\n",
    "\n",
    "#check for accuracity\n",
    "classification.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcd69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
